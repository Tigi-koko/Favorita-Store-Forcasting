{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tigi-koko/Favorita-Store-Forcasting/blob/main/Favorita_Corporaci%C3%B3n_Favorita_week3_xgb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q xgboost joblib mlflow scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad1mpc62OexY",
        "outputId": "30ccda43-cf82-42e9-e6c2-ddbcdf73dac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m752.6/752.6 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Mount Drive and set directories\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "BASE_DIR = '/content/drive/MyDrive/corporacion_favorita'\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
        "MODEL_DIR = os.path.join(BASE_DIR, 'model')\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, 'results')\n",
        "MLFLOW_DIR = os.path.join(BASE_DIR, 'mlruns')\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(MLFLOW_DIR, exist_ok=True)\n",
        "\n",
        "print(\"BASE_DIR:\", BASE_DIR)\n",
        "print(\"MLflow local store:\", MLFLOW_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp7RPVvNOpRf",
        "outputId": "50dfd25d-ca7f-4a54-c8f8-94dc50642e5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "BASE_DIR: /content/drive/MyDrive/corporacion_favorita\n",
            "MLflow local store: /content/drive/MyDrive/corporacion_favorita/mlruns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import xgboost as xgb\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.model_selection import TimeSeriesSplit"
      ],
      "metadata": {
        "id": "FaSfsAk1PrJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Load dataset (try common filenames)\n",
        "possible_files = [\n",
        "    os.path.join(DATA_DIR, 'data_clean.csv'),\n",
        "    os.path.join(DATA_DIR, 'train.csv'),\n",
        "    os.path.join(BASE_DIR, 'data_clean.csv'),\n",
        "    os.path.join(BASE_DIR, 'train.csv')\n",
        "]\n",
        "\n",
        "df = None\n",
        "for f in possible_files:\n",
        "    if os.path.exists(f):\n",
        "        print(\"Loading:\", f)\n",
        "        df = pd.read_csv(f, parse_dates=['date'], low_memory=False)\n",
        "        break\n",
        "if df is None:\n",
        "    raise FileNotFoundError(f\"No data file found. Checked: {possible_files}\")\n",
        "\n",
        "# Ensure 'unit_sales' exists\n",
        "if 'unit_sales' not in df.columns:\n",
        "    raise ValueError(\"CSV must contain 'unit_sales' column. Rename or provide correct file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSUV0tRNP3qA",
        "outputId": "47818bbb-42ff-46fa-a8e9-687785b2cb57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading: /content/drive/MyDrive/corporacion_favorita/data_clean.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Aggregate to daily total sales (simple baseline)\n",
        "daily = df.groupby('date', as_index=False)['unit_sales'].sum().rename(columns={'unit_sales':'daily_sales'})\n",
        "daily = daily.sort_values('date').reset_index(drop=True)\n",
        "daily['date'] = pd.to_datetime(daily['date'])\n",
        "daily = daily.set_index('date').asfreq('D')      # ensures contiguous dates\n",
        "daily['daily_sales'] = daily['daily_sales'].fillna(0)"
      ],
      "metadata": {
        "id": "USMZllRlQUiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Feature engineering (lags, rolling, date parts)\n",
        "def create_features(series):\n",
        "    df = series.to_frame(name='daily_sales').copy()\n",
        "    df['date'] = df.index\n",
        "    df['day'] = df.index.day\n",
        "    df['month'] = df.index.month\n",
        "    df['year'] = df.index.year\n",
        "    df['dayofweek'] = df.index.dayofweek\n",
        "    df['is_weekend'] = df['dayofweek'].isin([5,6]).astype(int)\n",
        "    df['weekofyear'] = df.index.isocalendar().week.astype(int)\n",
        "\n",
        "    for lag in [1,7,14,28]:\n",
        "        df[f'lag_{lag}'] = df['daily_sales'].shift(lag)\n",
        "    df['roll_7_mean'] = df['daily_sales'].shift(1).rolling(7).mean()\n",
        "    df['roll_14_mean'] = df['daily_sales'].shift(1).rolling(14).mean()\n",
        "    df['roll_28_mean'] = df['daily_sales'].shift(1).rolling(28).mean()\n",
        "    df = df.fillna(method='ffill').fillna(0)\n",
        "    return df\n",
        "\n",
        "df_feats = create_features(daily['daily_sales'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pywODXn8QtZG",
        "outputId": "e50d56cd-33fa-4888-997e-d124a91ab293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3870261902.py:17: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = df.fillna(method='ffill').fillna(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Train/test split: use 2014-01-01..2014-03-31 as test if present otherwise last 90 days\n",
        "start_2014 = pd.to_datetime('2014-01-01')\n",
        "end_2014 = pd.to_datetime('2014-03-31')\n",
        "\n",
        "if (df_feats.index.min() <= start_2014) and (df_feats.index.max() >= end_2014):\n",
        "    train_df = df_feats[df_feats.index < start_2014]\n",
        "    test_df = df_feats[(df_feats.index >= start_2014) & (df_feats.index <= end_2014)]\n",
        "    print(\"Using 2014-01-01..2014-03-31 as test set.\")\n",
        "else:\n",
        "    test_size = 90\n",
        "    train_df = df_feats.iloc[:-test_size]\n",
        "    test_df = df_feats.iloc[-test_size:]\n",
        "    print(f\"Using last {test_size} days as test set.\")\n",
        "\n",
        "TARGET = 'daily_sales'\n",
        "FEATURES = [c for c in df_feats.columns if c not in [TARGET, 'date']]\n",
        "\n",
        "X_train = train_df[FEATURES].values\n",
        "y_train = train_df[TARGET].values\n",
        "X_test = test_df[FEATURES].values\n",
        "y_test = test_df[TARGET].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEGwwD9RQyhv",
        "outputId": "8f760006-2d49-486e-9489-174b89ad957f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 2014-01-01..2014-03-31 as test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Scale\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_s = scaler.transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "scaler_path = os.path.join(MODEL_DIR, 'scaler.pkl')\n",
        "joblib.dump(scaler, scaler_path)\n",
        "print(\"Saved scaler:\", scaler_path)\n",
        "\n",
        "# 8) Metric functions (adds Bias, MAD, rMAD, MAPE)\n",
        "def bias(y_true, y_pred):\n",
        "    return np.mean(y_pred - y_true)\n",
        "\n",
        "def mad(y_true, y_pred):\n",
        "    return np.mean(np.abs(y_pred - y_true))\n",
        "\n",
        "def rmad(y_true, y_pred):\n",
        "    denom = np.mean(np.abs(y_true))\n",
        "    if denom == 0:\n",
        "        return np.nan\n",
        "    return np.mean(np.abs(y_pred - y_true)) / denom\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        res = np.abs((y_true - y_pred) / np.where(y_true==0, np.nan, y_true))\n",
        "    return np.nanmean(res) * 100\n",
        "\n",
        "def evaluate_all(y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    b = bias(y_true, y_pred)\n",
        "    _mad = mad(y_true, y_pred)\n",
        "    _rmad = rmad(y_true, y_pred)\n",
        "    _mape = mape(y_true, y_pred)\n",
        "    return {'RMSE': rmse, 'MAE': mae, 'Bias': b, 'MAD': _mad, 'rMAD': _rmad, 'MAPE': _mape}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm11q1hGQ6ET",
        "outputId": "6afda3e5-101d-4a5a-ffd6-aa3d5307c189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved scaler: /content/drive/MyDrive/corporacion_favorita/model/scaler.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Setup MLflow to use file store inside your Drive\n",
        "mlflow.set_tracking_uri(f\"file://{MLFLOW_DIR}\")\n",
        "experiment_name = \"favorita-week3-xgb\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "print(\"MLflow tracking URI:\", mlflow.get_tracking_uri())\n",
        "print(\"Experiment name:\", experiment_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZR1B4NFRCUb",
        "outputId": "7acec015-a8d6-45b5-8b33-f3f20f417e5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow tracking URI: file:///content/drive/MyDrive/corporacion_favorita/mlruns\n",
            "Experiment name: favorita-week3-xgb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10) Baseline model: load if exists else train a simple XGB with baseline params\n",
        "baseline_path = os.path.join(MODEL_DIR, 'xgb_baseline.pkl')\n",
        "baseline_params = dict(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, n_jobs= -1)\n",
        "\n",
        "if os.path.exists(baseline_path):\n",
        "    print(\"Loading existing baseline model:\", baseline_path)\n",
        "    baseline_model = joblib.load(baseline_path)\n",
        "else:\n",
        "    print(\"Training baseline XGBoost...\")\n",
        "    baseline_model = xgb.XGBRegressor(**baseline_params)\n",
        "    baseline_model.fit(X_train_s, y_train)\n",
        "    joblib.dump(baseline_model, baseline_path)\n",
        "    print(\"Saved baseline model:\", baseline_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tumXcKYNRNU3",
        "outputId": "e30c0826-a66d-4675-e1b3-dcfe82867097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing baseline model: /content/drive/MyDrive/corporacion_favorita/model/xgb_baseline.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11) Baseline evaluation\n",
        "y_pred_baseline = baseline_model.predict(X_test_s)\n",
        "metrics_baseline = evaluate_all(y_test, y_pred_baseline)\n",
        "print(\"Baseline metrics:\", metrics_baseline)\n",
        "\n",
        "# Save plot (actual vs predicted) to artifact and local folder\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(test_df.index, y_test, label='actual')\n",
        "plt.plot(test_df.index, y_pred_baseline, label='predicted')\n",
        "plt.title('Baseline: Actual vs Predicted')\n",
        "plt.legend()\n",
        "baseline_plot_path = os.path.join(OUTPUT_DIR, 'baseline_actual_vs_pred.png')\n",
        "plt.savefig(baseline_plot_path, bbox_inches='tight')\n",
        "plt.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s63xLFoqRU9K",
        "outputId": "c9d26b20-213a-4559-e505-5ffb8a1f6cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline metrics: {'RMSE': np.float64(21.764486769986913), 'MAE': 15.611165746053059, 'Bias': np.float64(3.5465384165445966), 'MAD': np.float64(15.611165746053059), 'rMAD': np.float64(0.31424847173893433), 'MAPE': np.float64(33.4146603522543)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12) Log baseline run to MLflow (params, metrics, artifact)\n",
        "with mlflow.start_run(run_name='baseline_xgb'):\n",
        "    mlflow.log_params({'model':'xgboost_baseline', **baseline_params})\n",
        "    mlflow.log_metrics(metrics_baseline)\n",
        "    mlflow.sklearn.log_model(baseline_model, artifact_path='model_baseline')\n",
        "    mlflow.log_artifact(baseline_plot_path, artifact_path='plots')\n",
        "    # log feature list and scaler as artifacts\n",
        "    features_path = os.path.join(OUTPUT_DIR, 'features_baseline.txt')\n",
        "    with open(features_path, 'w') as f:\n",
        "        f.write('\\n'.join(FEATURES))\n",
        "    mlflow.log_artifact(features_path, artifact_path='meta')\n",
        "    mlflow.log_artifact(scaler_path, artifact_path='meta')\n",
        "print(\"Baseline run logged to MLflow.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk8B1XtKRdp7",
        "outputId": "7d029618-d1f3-48b1-f759-e8c377d87c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/10/28 09:02:10 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "\u001b[31m2025/10/28 09:02:18 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline run logged to MLflow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13) Hyperparameter tuning with RandomizedSearchCV (time-series aware with TimeSeriesSplit)\n",
        "# Keep search small to run quickly on Colab\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'reg_alpha': [0, 0.1, 1.0],\n",
        "    'reg_lambda': [1.0, 2.0]\n",
        "}\n",
        "\n",
        "xgb_est = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=xgb_est,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,              # keep moderate; reduce to 8-10 for faster runs\n",
        "    cv=tscv,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=1\n",
        ")\n",
        "\n",
        "print(\"Starting RandomizedSearchCV (this can take several minutes)...\")\n",
        "search.fit(X_train_s, y_train)\n",
        "print(\"Best params from search:\", search.best_params_)\n",
        "best_params = search.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WlLvl9TRpHA",
        "outputId": "86495792-0678-4c3a-ea9e-927ff30d5343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting RandomizedSearchCV (this can take several minutes)...\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "Best params from search: {'subsample': 0.9, 'reg_lambda': 1.0, 'reg_alpha': 0, 'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.03, 'colsample_bytree': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14) Retrain best model on full training set\n",
        "best_model = xgb.XGBRegressor(**best_params, random_state=42, n_jobs=-1)\n",
        "best_model.fit(X_train_s, y_train)\n",
        "best_path = os.path.join(MODEL_DIR, 'xgb_tuned.pkl')\n",
        "joblib.dump(best_model, best_path)\n",
        "print(\"Saved tuned model:\", best_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT3P9TfZR7dx",
        "outputId": "40fb1040-d161-4ca4-e2a9-fed1e6f56fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved tuned model: /content/drive/MyDrive/corporacion_favorita/model/xgb_tuned.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 15) Evaluate tuned model on test set\n",
        "y_pred_best = best_model.predict(X_test_s)\n",
        "metrics_best = evaluate_all(y_test, y_pred_best)\n",
        "print(\"Tuned metrics:\", metrics_best)\n",
        "# Save tuned plot\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(test_df.index, y_test, label='actual')\n",
        "plt.plot(test_df.index, y_pred_best, label='predicted_tuned')\n",
        "plt.title('Tuned XGB: Actual vs Predicted')\n",
        "plt.legend()\n",
        "tuned_plot_path = os.path.join(OUTPUT_DIR, 'tuned_actual_vs_pred.png')\n",
        "plt.savefig(tuned_plot_path, bbox_inches='tight')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFhZCdROTdfs",
        "outputId": "fc2dfff7-b0c7-4aec-b753-30f96f959b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned metrics: {'RMSE': np.float64(24.51409477472981), 'MAE': 16.36393199496799, 'Bias': np.float64(4.80888311598036), 'MAD': np.float64(16.36393199496799), 'rMAD': np.float64(0.3294014492388994), 'MAPE': np.float64(35.188322013284186)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 16) Log tuned run to MLflow\n",
        "with mlflow.start_run(run_name='tuned_xgb'):\n",
        "    mlflow.log_params({'model':'xgboost_tuned', **best_params})\n",
        "    mlflow.log_metrics(metrics_best)\n",
        "    mlflow.sklearn.log_model(best_model, artifact_path='model_tuned')\n",
        "    mlflow.log_artifact(tuned_plot_path, artifact_path='plots')\n",
        "    # log artifacts: features, scaler\n",
        "    mlflow.log_artifact(features_path, artifact_path='meta')\n",
        "    mlflow.log_artifact(scaler_path, artifact_path='meta')\n",
        "print(\"Tuned run logged to MLflow.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3bhTNK1TfGB",
        "outputId": "52b4f6bb-0b91-4846-9fb7-b23cbcbbfa2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/10/28 09:02:39 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "\u001b[31m2025/10/28 09:02:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned run logged to MLflow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 17) Save feature list and pipeline metadata for next week\n",
        "features_file = os.path.join(MODEL_DIR, 'features.txt')\n",
        "with open(features_file, 'w') as f:\n",
        "    f.write('\\n'.join(FEATURES))\n",
        "\n",
        "pipeline_meta = {\n",
        "    'model_baseline': baseline_path,\n",
        "    'model_tuned': best_path,\n",
        "    'scaler': scaler_path,\n",
        "    'features': features_file,\n",
        "    'trained_until': str(train_df.index.max())\n",
        "}\n",
        "joblib.dump(pipeline_meta, os.path.join(MODEL_DIR, 'pipeline_meta.pkl'))\n",
        "print(\"Saved pipeline metadata:\", os.path.join(MODEL_DIR, 'pipeline_meta.pkl'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CgmlyUdTzp-",
        "outputId": "2f4d95dc-d3a8-49bd-e7d1-a19d50a6e41d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pipeline metadata: /content/drive/MyDrive/corporacion_favorita/model/pipeline_meta.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 18) Print helpful pointers and MLflow UI instructions\n",
        "print(\"\\n=== DONE ===\")\n",
        "print(\"Baseline metrics:\", metrics_baseline)\n",
        "print(\"Tuned metrics:\", metrics_best)\n",
        "print(\"Saved models to:\", baseline_path, best_path)\n",
        "print(\"Saved artifacts (plots, features) to:\", OUTPUT_DIR, MODEL_DIR)\n",
        "\n",
        "print(\"\\nTo view MLflow UI locally from Colab, run this command in a new cell (it will run a small server):\")\n",
        "print(\"!mlflow ui --backend-store-uri file://{mlruns} --port 5000\".format(mlruns=MLFLOW_DIR))\n",
        "print(\"Then open the forwarded port (or use ngrok) OR download the mlruns folder and view with MLflow locally.\")\n",
        "print(\"You can also explore the mlruns folder in Drive: \", MLFLOW_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZlRexuqT7JH",
        "outputId": "8483608e-158c-4d6e-979b-e1c322447164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DONE ===\n",
            "Baseline metrics: {'RMSE': np.float64(21.764486769986913), 'MAE': 15.611165746053059, 'Bias': np.float64(3.5465384165445966), 'MAD': np.float64(15.611165746053059), 'rMAD': np.float64(0.31424847173893433), 'MAPE': np.float64(33.4146603522543)}\n",
            "Tuned metrics: {'RMSE': np.float64(24.51409477472981), 'MAE': 16.36393199496799, 'Bias': np.float64(4.80888311598036), 'MAD': np.float64(16.36393199496799), 'rMAD': np.float64(0.3294014492388994), 'MAPE': np.float64(35.188322013284186)}\n",
            "Saved models to: /content/drive/MyDrive/corporacion_favorita/model/xgb_baseline.pkl /content/drive/MyDrive/corporacion_favorita/model/xgb_tuned.pkl\n",
            "Saved artifacts (plots, features) to: /content/drive/MyDrive/corporacion_favorita/results /content/drive/MyDrive/corporacion_favorita/model\n",
            "\n",
            "To view MLflow UI locally from Colab, run this command in a new cell (it will run a small server):\n",
            "!mlflow ui --backend-store-uri file:///content/drive/MyDrive/corporacion_favorita/mlruns --port 5000\n",
            "Then open the forwarded port (or use ngrok) OR download the mlruns folder and view with MLflow locally.\n",
            "You can also explore the mlruns folder in Drive:  /content/drive/MyDrive/corporacion_favorita/mlruns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60ac2792",
        "outputId": "f3f1558a-c16c-49ea-8b76-4b67283a1d2a"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Load your training data (replace with your actual dataset)\n",
        "# Using the same file used for initial data loading in cell CSUV0tRNP3qA\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/corporacion_favorita/data_clean.csv', parse_dates=['date'], low_memory=False)\n",
        "\n",
        "# Use the same feature list used before\n",
        "# Corrected the file path to match where it was saved in cell 0CgmlyUdTzp-\n",
        "features_file_path = '/content/drive/MyDrive/corporacion_favorita/model/features.txt'\n",
        "with open(features_file_path, 'r') as f:\n",
        "    features = f.read().splitlines()\n",
        "\n",
        "# Fit the scaler again\n",
        "scaler = StandardScaler()\n",
        "# Before fitting, ensure that the features in train_df are the same used for training\n",
        "# In this notebook, the features were created after aggregating to daily sales and feature engineering\n",
        "# The train_df used here is the original data_clean.csv, which doesn't have the engineered features\n",
        "# To correctly scale, we need to apply the same feature engineering steps to the loaded data\n",
        "# and then select the features used for training.\n",
        "\n",
        "# Re-applying the feature engineering function from cell pywODXn8QtZG\n",
        "def create_features(series):\n",
        "    df = series.to_frame(name='daily_sales').copy()\n",
        "    df['date'] = df.index\n",
        "    df['day'] = df.index.day\n",
        "    df['month'] = df.index.month\n",
        "    df['year'] = df.index.year\n",
        "    df['dayofweek'] = df.index.dayofweek\n",
        "    df['is_weekend'] = df['dayofweek'].isin([5,6]).astype(int)\n",
        "    df['weekofyear'] = df.index.isocalendar().week.astype(int)\n",
        "\n",
        "    for lag in [1,7,14,28]:\n",
        "        df[f'lag_{lag}'] = df['daily_sales'].shift(lag)\n",
        "    df['roll_7_mean'] = df['daily_sales'].shift(1).rolling(7).mean()\n",
        "    df['roll_14_mean'] = df['daily_sales'].shift(1).rolling(14).mean()\n",
        "    df['roll_28_mean'] = df['daily_sales'].shift(1).rolling(28).mean()\n",
        "    # Using .ffill() and .fillna(0) as in the original feature engineering cell\n",
        "    df = df.ffill().fillna(0)\n",
        "    return df\n",
        "\n",
        "# Aggregate the loaded data to daily total sales as done in cell USMZllRlQUiv\n",
        "daily_for_scaling = train_df.groupby('date', as_index=False)['unit_sales'].sum().rename(columns={'unit_sales':'daily_sales'})\n",
        "daily_for_scaling = daily_for_scaling.sort_values('date').reset_index(drop=True)\n",
        "daily_for_scaling['date'] = pd.to_datetime(daily_for_scaling['date'])\n",
        "daily_for_scaling = daily_for_scaling.set_index('date').asfreq('D')      # ensures contiguous dates\n",
        "daily_for_scaling['daily_sales'] = daily_for_scaling['daily_sales'].fillna(0)\n",
        "\n",
        "# Create features for scaling using the function\n",
        "df_feats_for_scaling = create_features(daily_for_scaling['daily_sales'])\n",
        "\n",
        "# Select only the features used for training\n",
        "X_for_scaling = df_feats_for_scaling[features]\n",
        "\n",
        "\n",
        "scaler.fit(X_for_scaling)\n",
        "\n",
        "# Save the scaler again properly\n",
        "# Using the correct path as in cell Nm11q1hGQ6ET and 0CgmlyUdTzp-\n",
        "scaler_path = '/content/drive/MyDrive/corporacion_favorita/model/scaler.pkl'\n",
        "import joblib # Changed to joblib for consistency with other cell\n",
        "joblib.dump(scaler, scaler_path)\n",
        "\n",
        "\n",
        "print(\"✅ Scaler recreated and saved successfully!\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Scaler recreated and saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4b5f3bb",
        "outputId": "e8c94cde-7f4e-47b3-a516-5e0600bd4dac"
      },
      "source": [
        "import joblib # Import joblib\n",
        "with open('/content/drive/MyDrive/corporacion_favorita/model/scaler.pkl', 'rb') as f:\n",
        "    scaler = joblib.load(f) # Use joblib.load\n",
        "print(\"Scaler now loads fine!\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaler now loads fine!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt0cV4VREVxf",
        "outputId": "4b5b5b8d-7436-492c-b175-11d6e4768d79"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==1.2.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "mJZ3_mLvEZXb",
        "outputId": "f19ece31-8edc-4561-9b5d-9361ea7d63cf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.2.2\n",
            "  Downloading scikit-learn-1.2.2.tar.gz (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.2.2) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.2.2) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.2.2) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.2.2) (3.6.0)\n",
            "Building wheels for collected packages: scikit-learn\n",
            "  Building wheel for scikit-learn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-learn: filename=scikit_learn-1.2.2-cp312-cp312-linux_x86_64.whl size=9847599 sha256=96c19a1f5abd65299079bd92ea86b4647c7b12bba48d0e3960a631893c61a85c\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/f8/77/ae90c181b806f450a6fec8c8f794594e7c92fa79d7ca27e656\n",
            "Successfully built scikit-learn\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cuml-cu12 25.6.0 requires scikit-learn>=1.5, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "imbalanced-learn 0.14.0 requires scikit-learn<2,>=1.4.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              },
              "id": "ebfc003943d8481f86bf35b810d3ccea"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}