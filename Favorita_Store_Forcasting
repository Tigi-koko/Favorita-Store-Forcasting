{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tigi-koko/Favorita-Store-Forcasting/blob/main/Favorita_Store_Forcasting\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ6HtdYrVgEd",
        "outputId": "c0a94f69-5d4c-43b9-f5ff-eb3bc1be79e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WYAopvyzVpCj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9161c64-c944-4532-ca7e-1ee53a63139b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m8.0/10.1 MB\u001b[0m \u001b[31m241.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m242.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m142.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m290.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m145.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#Install necessary packages\n",
        "\n",
        "!pip install streamlit pyngrok --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5cXibkRXV6Go"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/corporacion_favorita/model/scaler.pkl'\n",
        "data_path = '/content/drive/MyDrive/corporacion_favorita/data_clean.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LJmPUcIUv-l",
        "outputId": "f21f611d-12a5-414c-ffb9-6adf6f0ee72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "#Write your Streamlit app to app.py\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "073697e4"
      },
      "outputs": [],
      "source": [
        "mlflow_project_path = '/content/drive/MyDrive/corporacion_favorita/mlflow_results'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62f9d932",
        "outputId": "1b9ac730-b6c7-4986-d4c6-0aa24294ae2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project structure created and files updated.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the base project directory (adjust if needed)\n",
        "project_root = '/content/drive/MyDrive/corporacion_favorita' # Assuming this is your project root\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(os.path.join(project_root, 'app'), exist_ok=True)\n",
        "os.makedirs(os.path.join(project_root, 'model'), exist_ok=True)\n",
        "os.makedirs(os.path.join(project_root, 'data'), exist_ok=True)\n",
        "\n",
        "# Create __init__.py files\n",
        "open(os.path.join(project_root, 'app', '__init__.py'), 'a').close()\n",
        "open(os.path.join(project_root, 'model', '__init__.py'), 'a').close()\n",
        "open(os.path.join(project_root, 'data', '__init__.py'), 'a').close()\n",
        "\n",
        "# Rename app.py to main.py (assuming app.py is in the project root initially)\n",
        "# If app.py is in the 'app' directory, adjust the source path\n",
        "if os.path.exists('/content/app.py'):\n",
        "    shutil.move('/content/app.py', os.path.join(project_root, 'app', 'main.py'))\n",
        "# If model.py exists in the 'model' directory, rename it\n",
        "if os.path.exists(os.path.join(project_root, 'model', 'model.py')):\n",
        "    shutil.move(os.path.join(project_root, 'model', 'model.py'), os.path.join(project_root, 'model', 'model_utils.py'))\n",
        "\n",
        "\n",
        "# Create requirements.txt\n",
        "open(os.path.join(project_root, 'requirements.txt'), 'a').close()\n",
        "\n",
        "print(\"Project structure created and files updated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "808f2228",
        "outputId": "3d5e57dd-befe-4021-f018-554c320cab66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/corporacion_favorita/requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/drive/MyDrive/corporacion_favorita/requirements.txt\n",
        "streamlit\n",
        "pandas\n",
        "numpy\n",
        "scikit-learn\n",
        "tensorflow\n",
        "requests\n",
        "gdown\n",
        "mlflow\n",
        "matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9f25393a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52908478-4654-4efd-e4e9-f64a21072a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m752.6/752.6 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/drive/MyDrive/corporacion_favorita/requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Bjol6qVSXlpD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Project root (…/corporacion_favorita)\n",
        "BASE_DIR = '/content/drive/MyDrive/corporacion_favorita'\n",
        "\n",
        "# --- MLflow store & model location ---\n",
        "# Points MLflow at your local runs folder (copied from Colab/Drive).\n",
        "# Must be an ABSOLUTE file URI (note the three slashes after file:).\n",
        "# TODO: Update this path to the absolute path of your local mlflow_results folder.\n",
        "MLFLOW_TRACKING_URI = \"file:///Users/deb/Documents/corporacion_favorita/mlflow_results\"\n",
        "\n",
        "# Model to load from MLflow. Add your own run_id here. We saw how to get it before.\n",
        "# TODO: Replace <your_run_id> with the actual run ID from your MLflow experiment.\n",
        "MODEL_URI = \"runs:/26a599ff4fe14c7fb552e9ed53534ecb/model\"\n",
        "\n",
        "# --- Local fallbacks (used if MLflow load fails) ---\n",
        "MODEL_PATH    = os.path.join(BASE_DIR, \"model\", \"lstm_model.keras\")\n",
        "SCALER_PATH   = os.path.join(BASE_DIR, \"model\", \"scaler.pkl\")\n",
        "FEATURES_JSON = os.path.join(BASE_DIR, \"model\", \"feature_cols.json\")\n",
        "\n",
        "# --- Model window length used in feature windowing ---\n",
        "SEQ_LEN = 60\n",
        "\n",
        "# --- Google Drive file IDs for metadata + filtered training DF ---\n",
        "FILE_IDS = {\n",
        "    \"holiday_events\": \"1RMjSuqHXHTwAw_PGD5XVjhA3agaAGHDH\",\n",
        "    \"items\":          \"1ogMRixVhNY6XOJtIRtkRllyOyzw1nqya\",\n",
        "    \"oil\":            \"1Q59vk2v4WQ-Rpc9t2nqHcsZM3QWGFje_\",\n",
        "    \"stores\":         \"1Ei0MUXmNhmOcmrlPad8oklnFEDM95cDi\",\n",
        "    \"transactions\":   \"1PW5LnAEAiL43fI5CRDn_h6pgDG5rtBW_\",\n",
        "    # TODO: Replace \"1BSwHTdLTrfDzSlunnTjZk8_bfKZf4EHk\" with the file ID of your pickled DataFrame from Sprint 3.\n",
        "    \"train\": \"1BSwHTdLTrfDzSlunnTjZk8_bfKZf4EHk\" # Replace with your Sprint 3 pickled df ID;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KwYteWOVYREs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests, io\n",
        "# from app.config import FILE_IDS # Removed as FILE_IDS is defined in a previous cell\n",
        "\n",
        "# ---------- Drive helpers ----------\n",
        "\n",
        "def make_drive_url(file_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Build a direct-download URL for a Google Drive file ID.\n",
        "    NOTE: The Drive file must be shared as “Anyone with the link can view”.\n",
        "    \"\"\"\n",
        "    return f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "def load_csv_from_url(url: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Download a CSV from a direct URL and read it into a DataFrame.\n",
        "    Uses requests for HTTP and StringIO to feed text to pandas.\n",
        "    \"\"\"\n",
        "    r = requests.get(url)\n",
        "    r.raise_for_status()                 # raise if HTTP request failed (e.g., 403/404)\n",
        "    return pd.read_csv(io.StringIO(r.text))\n",
        "\n",
        "# ---------- Data loading ----------\n",
        "\n",
        "def read_metadata_files(file_ids=FILE_IDS):\n",
        "    \"\"\"\n",
        "    Load the 5 metadata tables used for context/joins:\n",
        "      - holiday_events, items, oil, stores, transactions\n",
        "    Returns 5 DataFrames in that order.\n",
        "    \"\"\"\n",
        "    df_holiday_events, df_items, df_oil, df_stores, df_transactions = read_metadata_files(file_ids)\n",
        "\n",
        "    # Load the filtered time series (single store/item) from a pickled DataFrame on Drive.\n",
        "    # IMPORTANT: The Drive file must be public to read via direct link.\n",
        "    # Using a safer method to handle potential HTML responses from Drive links\n",
        "    url = make_drive_url(file_ids[\"train\"])\n",
        "    r = requests.get(url, allow_redirects=True); r.raise_for_status()\n",
        "    if r.content[:1] == b\"<\":\n",
        "        raise RuntimeError(\"Drive returned HTML. Check sharing or file ID.\")\n",
        "    df_filtered = pd.read_pickle(io.BytesIO(r.content))\n",
        "\n",
        "\n",
        "    return df_stores, df_items, df_transactions, df_oil, df_holiday_events, df_filtered\n",
        "\n",
        "# ---------- Feature engineering ----------\n",
        "\n",
        "def creating_features(df_filtered: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add features the LSTM expects.\n",
        "    Assumes:\n",
        "      - df_filtered has a daily DatetimeIndex\n",
        "      - 'unit_sales' column exists\n",
        "    \"\"\"\n",
        "    df = df_filtered.copy()\n",
        "\n",
        "    # Target lags: yesterday, last week, last month (by day count)\n",
        "    df[\"lag_1\"]  = df[\"unit_sales\"].shift(1)\n",
        "    df[\"lag_7\"]  = df[\"unit_sales\"].shift(7)\n",
        "    df[\"lag_30\"] = df[\"unit_sales\"].shift(30)\n",
        "    df.dropna(inplace=True)  # drop rows made NaN by the shifts above\n",
        "\n",
        "    # Calendar features derived from the index\n",
        "    df[\"day_of_week\"] = df.index.dayofweek     # 0=Mon ... 6=Sun\n",
        "    df[\"month\"]       = df.index.month\n",
        "    df[\"is_weekend\"]  = (df[\"day_of_week\"] >= 5).astype(int)\n",
        "\n",
        "    # Rolling stats on the target (shift by 1 to avoid peeking into the current day)\n",
        "    df[\"rolling_mean_7\"] = df[\"unit_sales\"].shift(1).rolling(window=7).mean()\n",
        "    df[\"rolling_std_7\"]  = df[\"unit_sales\"].shift(1).rolling(window=7).std()\n",
        "\n",
        "    # Drop any rows still NaN due to rolling window warm-up\n",
        "    df.dropna(inplace=True)\n",
        "    return df\n",
        "\n",
        "def preprocess_input_data(df_filtered: pd.DataFrame, pickled: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepare a DataFrame for inference.\n",
        "\n",
        "    If 'pickled' is True:\n",
        "      - assume df_filtered is already daily, indexed by date, and aggregated.\n",
        "\n",
        "    If 'pickled' is False:\n",
        "      - parse 'date' column to datetime\n",
        "      - aggregate to daily totals\n",
        "      - set daily frequency and fill missing days with zeros\n",
        "      - then build features\n",
        "    \"\"\"\n",
        "    if not pickled:\n",
        "        df = df_filtered.copy()\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "        # Aggregate to daily totals (numeric_only guards against non-numeric columns)\n",
        "        df = df.groupby(\"date\").sum(numeric_only=True)[\"unit_sales\"].reset_index()\n",
        "        df.set_index(\"date\", inplace=True)\n",
        "        df = df.asfreq(\"D\").fillna(0)  # fill gaps with 0 sales\n",
        "    else:\n",
        "        df = df_filtered\n",
        "\n",
        "    return creating_features(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "84ogfg22Y8ji"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pickle\n",
        "import mlflow\n",
        "import mlflow.keras\n",
        "import tensorflow as tf\n",
        "\n",
        "# from app.config import ( # Removed incorrect import\n",
        "#     MLFLOW_TRACKING_URI,\n",
        "#     MODEL_URI,\n",
        "#     MODEL_PATH,\n",
        "#     SCALER_PATH,\n",
        "#     FEATURES_JSON,\n",
        "# )\n",
        "\n",
        "def _mlflow_setup():\n",
        "    \"\"\"\n",
        "    Point MLflow at your local tracking store so `runs:/…` can be resolved.\n",
        "    \"\"\"\n",
        "    # Use the variable defined in cell Bjol6qVSXlpD\n",
        "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "\n",
        "def load_lstm_model():\n",
        "    \"\"\"\n",
        "    Load the Keras model.\n",
        "    - First try MLflow with MODEL_URI (runs:/<run_id>/model).\n",
        "    - If that fails (e.g., wrong URI/path), fall back to the local MODEL_PATH.\n",
        "    \"\"\"\n",
        "    _mlflow_setup()\n",
        "    try:\n",
        "        # Use the variable defined in cell Bjol6qVSXlpD\n",
        "        return mlflow.keras.load_model(MODEL_URI)\n",
        "    except Exception as e:\n",
        "        print(f\"[MLflow load failed] {e}\\\\nFalling back to local MODEL_PATH: {MODEL_PATH}\")\n",
        "        # Use the variable defined in cell Bjol6qVSXlpD\n",
        "        return tf.keras.models.load_model(MODEL_PATH)\n",
        "\n",
        "def load_scaler_and_features():\n",
        "    \"\"\"\n",
        "    Load the preprocessing artifacts required for inference:\n",
        "    - scaler.pkl : the fitted scaler used during training\n",
        "    - feature_cols.json : the exact feature order expected by the model\n",
        "    \"\"\"\n",
        "    # Use the variables defined in cell Bjol6qVSXlpD\n",
        "    with open(SCALER_PATH, \"rb\") as f:\n",
        "        scaler = pickle.load(f)\n",
        "    with open(FEATURES_JSON) as f:\n",
        "        feature_cols = json.load(f)\n",
        "    return scaler, feature_cols\n",
        "\n",
        "def predict_scaled(model, X_3d):\n",
        "    \"\"\"\n",
        "    Run the model on a 3D tensor shaped (batch, seq_len, n_features).\n",
        "    Returns a scalar (float) prediction in the **scaled space**.\n",
        "    \"\"\"\n",
        "    return model.predict(X_3d, verbose=0).ravel()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nrAqMOYOq1mc",
        "outputId": "00c0e174-3dca-4d84-bf42-91c72bfc03ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/corporacion_favorita/model/scaler.pkl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\"/content/drive/MyDrive/corporacion_favorita/model/scaler.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb56698c",
        "outputId": "483de8b8-19f4-4c8e-e2b4-d5fd0401983e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-10-28 11:15:26.740 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:26.745 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:27.534 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-10-28 11:15:27.538 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:27.540 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:27.542 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:27.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:27.545 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.541 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.542 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.544 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.545 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.546 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.548 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.549 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.550 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.550 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.552 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.554 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.555 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.556 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.557 Session state does not function when running a script without `streamlit run`\n",
            "2025-10-28 11:15:46.558 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.558 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.560 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.561 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.562 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.563 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.564 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.565 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-28 11:15:46.566 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MLflow load failed] Run '26a599ff4fe14c7fb552e9ed53534ecb' not found\\nFalling back to local MODEL_PATH: /content/drive/MyDrive/corporacion_favorita/model/lstm_model.keras\n"
          ]
        }
      ],
      "source": [
        "# --- Make project packages importable (app, data, model) ---\n",
        "# Add the project root (…/corporacion_favorita) to sys.path so we can do:\n",
        "#   from app.config import ...\n",
        "#   from data.data_utils import ...\n",
        "#   from model.model_utils import ...\n",
        "import sys, os\n",
        "# Use the known project root path instead of __file__\n",
        "ROOT = '/content/drive/MyDrive/corporacion_favorita'\n",
        "if ROOT not in sys.path:\n",
        "    sys.path.insert(0, ROOT)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Core libs and UI\n",
        "import streamlit as st\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests, io # Added imports for data loading\n",
        "\n",
        "# Project config and modules\n",
        "# Importing from modules within the project structure\n",
        "from app.config import FILE_IDS, SEQ_LEN          # FILE_IDS has Drive file IDs; SEQ_LEN = LSTM window\n",
        "# Corrected import for data utility functions (removed import, functions called directly)\n",
        "# from data.data_utils import load_data, preprocess_input_data\n",
        "# Corrected import for model utility functions (removed import, functions called directly)\n",
        "# from model.model_utils import load_lstm_model, load_scaler_and_features, predict_scaled\n",
        "\n",
        "# Include data loading and preprocessing functions directly\n",
        "# ---------- Drive helpers ----------\n",
        "\n",
        "def make_drive_url(file_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Build a direct-download URL for a Google Drive file ID.\n",
        "    NOTE: The Drive file must be shared as “Anyone with the link can view”.\n",
        "    \"\"\"\n",
        "    return f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "def load_csv_from_url(url: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Download a CSV from a direct URL and read it into a DataFrame.\n",
        "    Uses requests for HTTP and StringIO to feed text to pandas.\n",
        "    \"\"\"\n",
        "    r = requests.get(url)\n",
        "    r.raise_for_status()                 # raise if HTTP request failed (e.g., 403/404)\n",
        "    return pd.read_csv(io.StringIO(r.text))\n",
        "\n",
        "# ---------- Data loading ----------\n",
        "\n",
        "def read_metadata_files(file_ids=FILE_IDS):\n",
        "    \"\"\"\n",
        "    Load the 5 metadata tables used for context/joins:\n",
        "      - holiday_events, items, oil, stores, transactions\n",
        "    Returns 5 DataFrames in that order.\n",
        "    \"\"\"\n",
        "    df_holiday_events = load_csv_from_url(make_drive_url(file_ids[\"holiday_events\"]))\n",
        "    df_items          = load_csv_from_url(make_drive_url(file_ids[\"items\"]))\n",
        "    df_oil            = load_csv_from_url(make_drive_url(file_ids[\"oil\"]))\n",
        "    df_stores         = load_csv_from_url(make_drive_url(file_ids[\"stores\"]))\n",
        "    df_transactions   = load_csv_from_url(make_drive_url(file_ids[\"transactions\"]))\n",
        "    return df_holiday_events, df_items, df_oil, df_stores, df_transactions\n",
        "\n",
        "def load_data(file_ids=FILE_IDS):\n",
        "    \"\"\"\n",
        "    Download metadata CSVs and the filtered training series (pickled DataFrame).\n",
        "\n",
        "    Returns (in order):\n",
        "        df_stores, df_items, df_transactions, df_oil, df_holiday_events, df_filtered\n",
        "    \"\"\"\n",
        "    # Load the five metadata tables\n",
        "    df_holiday_events, df_items, df_oil, df_stores, df_transactions = read_metadata_files(file_ids)\n",
        "\n",
        "    # Load the filtered time series (single store/item) from a pickled DataFrame on Drive.\n",
        "    # IMPORTANT: The Drive file must be public to read via direct link.\n",
        "    # Using a safer method to handle potential HTML responses from Drive links\n",
        "    url = make_drive_url(file_ids[\"train\"])\n",
        "    r = requests.get(url, allow_redirects=True); r.raise_for_status()\n",
        "    if r.content[:1] == b\"<\":\n",
        "        raise RuntimeError(\"Drive returned HTML. Check sharing or file ID.\")\n",
        "    df_filtered = pd.read_pickle(io.BytesIO(r.content))\n",
        "\n",
        "\n",
        "    return df_stores, df_items, df_transactions, df_oil, df_holiday_events, df_filtered\n",
        "\n",
        "# ---------- Feature engineering ----------\n",
        "\n",
        "def creating_features(df_filtered: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add features the LSTM expects.\n",
        "    Assumes:\n",
        "      - df_filtered has a daily DatetimeIndex\n",
        "      - 'unit_sales' column exists\n",
        "    \"\"\"\n",
        "    df = df_filtered.copy()\n",
        "\n",
        "    # Target lags: yesterday, last week, last month (by day count)\n",
        "    df[\"lag_1\"]  = df[\"unit_sales\"].shift(1)\n",
        "    df[\"lag_7\"]  = df[\"unit_sales\"].shift(7)\n",
        "    df[\"lag_30\"] = df[\"unit_sales\"].shift(30)\n",
        "    df.dropna(inplace=True)  # drop rows made NaN by the shifts above\n",
        "\n",
        "    # Calendar features derived from the index\n",
        "    df[\"day_of_week\"] = df.index.dayofweek     # 0=Mon ... 6=Sun\n",
        "    df[\"month\"]       = df.index.month\n",
        "    df[\"is_weekend\"]  = (df[\"day_of_week\"] >= 5).astype(int)\n",
        "\n",
        "    # Rolling stats on the target (shift by 1 to avoid peeking into the current day)\n",
        "    df[\"rolling_mean_7\"] = df[\"unit_sales\"].shift(1).rolling(window=7).mean()\n",
        "    df[\"rolling_std_7\"]  = df[\"unit_sales\"].shift(1).rolling(window=7).std()\n",
        "\n",
        "    # Drop any rows still NaN due to rolling window warm-up\n",
        "    df.dropna(inplace=True)\n",
        "    return df\n",
        "\n",
        "def preprocess_input_data(df_filtered: pd.DataFrame, pickled: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepare a DataFrame for inference.\n",
        "\n",
        "    If 'pickled' is True:\n",
        "      - assume df_filtered is already daily, indexed by date, and aggregated.\n",
        "\n",
        "    If 'pickled' is False:\n",
        "      - parse 'date' column to datetime\n",
        "      - aggregate to daily totals (numeric_only guards against non-numeric columns)\n",
        "      - set daily frequency and fill missing days with zeros\n",
        "      - then build features\n",
        "    \"\"\"\n",
        "    if not pickled:\n",
        "        df = df_filtered.copy()\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "        # Aggregate to daily totals (numeric_only guards against non-numeric columns)\n",
        "        df = df.groupby(\"date\").sum(numeric_only=True)[\"unit_sales\"].reset_index()\n",
        "        df.set_index(\"date\", inplace=True)\n",
        "        df = df.asfreq(\"D\").fillna(0)  # fill gaps with 0 sales\n",
        "    else:\n",
        "        df = df_filtered\n",
        "\n",
        "    return creating_features(df)\n",
        "\n",
        "# Include model loading and prediction functions directly\n",
        "# from model.model_utils import load_lstm_model, load_scaler_and_features, predict_scaled # Removed import\n",
        "# Including model loading and prediction functions directly\n",
        "import json\n",
        "import pickle\n",
        "import mlflow\n",
        "import mlflow.keras\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming these config variables are available in the environment or imported elsewhere\n",
        "# from app.config import MLFLOW_TRACKING_URI, MODEL_URI, MODEL_PATH, SCALER_PATH, FEATURES_JSON\n",
        "\n",
        "# Configuration variables (copied from cell Bjol6qVSXlpD)\n",
        "BASE_DIR = '/content/drive/MyDrive/corporacion_favorita' # Assuming this is your project root\n",
        "MLFLOW_TRACKING_URI = \"file:///content/drive/MyDrive/corporacion_favorita/mlflow_results\" # TODO: Update this path to the absolute path of your local mlflow_results folder.\n",
        "MODEL_URI = \"runs:/26a599ff4fe14c7fb552e9ed53534ecb/model\" # TODO: Replace <your_run_id> with the actual run ID from your MLflow experiment.\n",
        "# Corrected directory name from \"models\" to \"model\"\n",
        "MODEL_PATH    = os.path.join(BASE_DIR, \"model\", \"lstm_model.keras\")\n",
        "SCALER_PATH   = os.path.join(BASE_DIR, \"model\", \"scaler.pkl\")\n",
        "FEATURES_JSON = os.path.join(BASE_DIR, \"model\", \"feature_cols.json\")\n",
        "\n",
        "def _mlflow_setup():\n",
        "    \"\"\"\n",
        "    Point MLflow at your local tracking store so `runs:/…` can be resolved.\n",
        "    \"\"\"\n",
        "    # Use the variable defined here in app/main.py\n",
        "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "\n",
        "def load_lstm_model():\n",
        "    \"\"\"\n",
        "    Load the Keras model.\n",
        "    - First try MLflow with MODEL_URI (runs:/<run_id>/model).\n",
        "    - If that fails (e.g., wrong URI/path), fall back to the local MODEL_PATH.\n",
        "    \"\"\"\n",
        "    _mlflow_setup()\n",
        "    try:\n",
        "        # Use the variable defined here in app/main.py\n",
        "        return mlflow.keras.load_model(MODEL_URI)\n",
        "    except Exception as e:\n",
        "        print(f\"[MLflow load failed] {e}\\\\nFalling back to local MODEL_PATH: {MODEL_PATH}\")\n",
        "        # Use the variable defined here in app/main.py\n",
        "        return tf.keras.models.load_model(MODEL_PATH)\n",
        "\n",
        "def load_scaler_and_features():\n",
        "    \"\"\"\n",
        "    Load the preprocessing artifacts required for inference:\n",
        "    - scaler.pkl : the fitted scaler used during training\n",
        "    - feature_cols.json : the exact feature order expected by the model\n",
        "    \"\"\"\n",
        "    # Use the variables defined here in app/main.py\n",
        "    with open(SCALER_PATH, \"rb\") as f:\n",
        "        scaler = pickle.load(f)\n",
        "    with open(FEATURES_JSON) as f:\n",
        "        feature_cols = json.load(f)\n",
        "    return scaler, feature_cols\n",
        "\n",
        "def predict_scaled(model, X_3d):\n",
        "    \"\"\"\n",
        "    Run the model on a 3D tensor shaped (batch, seq_len, n_features).\n",
        "    Returns a scalar (float) prediction in the **scaled space**.\n",
        "    \"\"\"\n",
        "    return model.predict(X_3d, verbose=0).ravel()[0]\n",
        "\n",
        "\n",
        "# Fixed series selection (the pickled df is already filtered to this pair)\n",
        "# These are just displayed in the UI; the actual df is already filtered.\n",
        "FIXED_STORE_ID = 44\n",
        "FIXED_ITEM_ID  = 1047679\n",
        "\n",
        "# Streamlit page setup\n",
        "st.set_page_config(page_title=\"Corporación Favorita — Sales Forecasting\", layout=\"wide\")\n",
        "\n",
        "# ---------- Helpers for multi-day forecasting ----------\n",
        "\n",
        "def _predict_next(model, scaler, feature_cols, history, seq_len):\n",
        "    \"\"\"\n",
        "    Predict next day's unit_sales using the last `seq_len` rows from `history`.\n",
        "\n",
        "    Steps:\n",
        "      1) take last seq_len rows and select the trained feature columns\n",
        "      2) scale with the saved scaler\n",
        "      3) reshape to 3D (batch, seq_len, n_features) and run the model\n",
        "      4) inverse-scale ONLY the target (assumed to be the FIRST column)\n",
        "    \"\"\"\n",
        "    window = history.tail(seq_len)\n",
        "    X_win = window[feature_cols].to_numpy()\n",
        "    X_scaled = scaler.transform(X_win)\n",
        "    # The pickled model might expect a different input shape (e.g., 2D instead of 3D)\n",
        "    # This part might need significant adjustment based on the pickled model type.\n",
        "    # For now, we assume it takes a 3D input like the LSTM.\n",
        "    X_3d = X_scaled.reshape(1, SEQ_LEN, len(feature_cols))\n",
        "    y_scaled = predict_scaled(model, X_3d)\n",
        "\n",
        "    # Inverse-scale just the target: place the scaled y in column 0, zeros elsewhere.\n",
        "    pad = np.zeros((1, len(feature_cols)))\n",
        "    pad[:, 0] = y_scaled\n",
        "    y = float(scaler.inverse_transform(pad)[0, 0])\n",
        "    return y\n",
        "\n",
        "def forecast_horizon(model, scaler, feature_cols, feats, start_date, seq_len, horizon=7):\n",
        "    \"\"\"\n",
        "    Autoregressive forecast for `horizon` days, starting the day *after* `start_date`.\n",
        "    Feed each prediction back into the working series to produce the next step.\n",
        "\n",
        "    `feats` is the engineered feature DataFrame with a DatetimeIndex and a 'unit_sales' column.\n",
        "    \"\"\"\n",
        "    # Work on a copy up to the chosen cut-off date\n",
        "    work = feats.loc[:start_date].copy()\n",
        "    preds = []\n",
        "    current = pd.to_datetime(start_date)\n",
        "\n",
        "    for _ in range(horizon):\n",
        "        # 1) Predict the next day using the current history window\n",
        "        y = _predict_next(model, scaler, feature_cols, work, seq_len)\n",
        "        next_date = current + pd.Timedelta(days=1)\n",
        "        preds.append((next_date, y))\n",
        "\n",
        "        # 2) Create the new row of engineered features for `next_date`\n",
        "        #    Note: rolling features are computed from past ACTUALS ONLY (no leakage).\n",
        "        last = work[\"unit_sales\"].iloc[-1]\n",
        "        last7  = work[\"unit_sales\"].tail(7)\n",
        "        last30 = work[\"unit_sales\"].tail(30)\n",
        "        new = {\n",
        "            \"unit_sales\":      y,                                # we append the prediction as if it were the next actual\n",
        "            \"lag_1\":           last,                             # yesterday's actual\n",
        "            \"lag_7\":           (work[\"unit_sales\"].iloc[-7]  if len(work) >= 7  else last),\n",
        "            \"lag_30\":          (work[\"unit_sales\"].iloc[-30] if len(work) >= 30 else last),\n",
        "            \"day_of_week\":     next_date.dayofweek,\n",
        "            \"month\":           next_date.month,\n",
        "            \"is_weekend\":      1 if next_date.dayofweek >= 5 else 0,\n",
        "            # Training used shift(1).rolling(7) → compute from past actuals only (no inclusion of y)\n",
        "            \"rolling_mean_7\":  last7.mean() if len(last7) >= 1 else np.nan,\n",
        "            \"rolling_std_7\":   last7.std(ddof=1) if len(last7) >= 2 else 0.0,\n",
        "        }\n",
        "        # Append the synthetic row to extend the working history and move forward one day\n",
        "        work = pd.concat([work, pd.DataFrame([new], index=[next_date])])\n",
        "        current = next_date\n",
        "\n",
        "    # Collect predictions into a DataFrame indexed by date\n",
        "    fcst = pd.DataFrame(preds, columns=[\"date\", \"prediction\"]).set_index(\"date\")\n",
        "    return fcst, work\n",
        "\n",
        "# ---------- App ----------\n",
        "\n",
        "def main():\n",
        "    st.title(\"Corporación Favorita — Sales Forecasting\")\n",
        "    st.caption(f\"Series: Store {FIXED_STORE_ID} · Item {FIXED_ITEM_ID} (fixed)\")\n",
        "\n",
        "    # 1) Data → engineered features\n",
        "    # Load metadata + filtered series from Drive, then build the LSTM features.\n",
        "    try:\n",
        "        _stores, _items, _tx, _oil, _hol, df_train = load_data(FILE_IDS)\n",
        "        feats = preprocess_input_data(df_train)  # adds lags/rolling/calendar features\n",
        "    except Exception as e:\n",
        "        # If you see this: check Drive sharing/IDs; the 'train' file must allow \"Anyone with the link can view\".\n",
        "        st.error(f\"Failed to load data. Check Drive sharing and FILE_IDS. Details:\\n{e}\")\n",
        "        st.stop() # Stop execution if data loading fails\n",
        "\n",
        "    # 2) Model + scaler + feature order\n",
        "    # Try to load from the pickled model file.\n",
        "    try:\n",
        "        model = load_lstm_model() # Changed from load_model back to load_lstm_model to use MLflow/keras loading\n",
        "        scaler, feature_cols = load_scaler_and_features()\n",
        "    except Exception as e:\n",
        "        st.error(f\"Failed to load model or artifacts. Details:\\n{e}\")\n",
        "        st.stop() # Stop execution if model/artifact loading fails\n",
        "\n",
        "    # 3) Date & horizon selection\n",
        "    # Choose a cut-off date (use history up to this date). Forecast starts the next day.\n",
        "    default_date = datetime.date(2014, 3, 1)\n",
        "    date = st.date_input(\n",
        "        \"Forecast cut‑off (use history up to this date)\",\n",
        "        value=default_date,\n",
        "        min_value=feats.index.min().date(),\n",
        "        max_value=feats.index.max().date(),\n",
        "    )\n",
        "    ts = pd.to_datetime(date)\n",
        "\n",
        "    # Single day vs multi-day (N) selection\n",
        "    mode = st.radio(\"Forecast mode\", [\"Single day\", \"Next N days\"], horizontal=True)\n",
        "    horizon = st.slider(\"N days\", 1, 30, 7) if mode == \"Next N days\" else 1\n",
        "\n",
        "    # 4) Predict\n",
        "    if st.button(\"Get Forecast\"):\n",
        "        # Validate the chosen date and ensure we have at least SEQ_LEN days of history\n",
        "        if ts not in feats.index:\n",
        "            st.error(\"Date out of range.\"); st.stop()\n",
        "        window = feats.loc[:ts].tail(SEQ_LEN)\n",
        "        if len(window) < SEQ_LEN:\n",
        "            st.error(f\"Need {SEQ_LEN} days of history before {date}.\"); st.stop()\n",
        "\n",
        "        if horizon == 1:\n",
        "            # Single-day forecast (original behavior)\n",
        "            X_win = window[feature_cols].to_numpy()\n",
        "            X_scaled = scaler.transform(X_win)\n",
        "            # The pickled model might expect a different input shape (e.g., 2D instead of 3D)\n",
        "            # This part might need significant adjustment based on the pickled model type.\n",
        "            X_input = X_scaled.reshape(1, SEQ_LEN, len(feature_cols)) # Assuming 3D input like LSTM\n",
        "            y_pred_scaled = predict_scaled(model, X_input)\n",
        "\n",
        "            # Inverse-scale only the target dimension\n",
        "            pad = np.zeros((1, len(feature_cols))); pad[:, 0] = y_pred_scaled\n",
        "            y_pred = float(scaler.inverse_transform(pad)[0, 0])\n",
        "\n",
        "            st.success(f\"Predicted sales for {date}: {y_pred:.2f}\")\n",
        "            # For consistency with plotting/table, put the result on the NEXT day\n",
        "            fcst = pd.DataFrame({\"prediction\": [y_pred]},\n",
        "                                index=[ts + pd.Timedelta(days=1)])\n",
        "            work = feats\n",
        "        else:\n",
        "            # Autoregressive N-day forecast\n",
        "            fcst, work = forecast_horizon(model, scaler, feature_cols, feats, ts, SEQ_LEN, horizon)\n",
        "            st.success(f\"Predicted {horizon} days: {fcst.index[0].date()} → {fcst.index[-1].date()}.\")\n",
        "\n",
        "        # 5) Plot: last ~6 months of history + forecast overlay\n",
        "        hist = feats.loc[max(feats.index.min(), ts - pd.Timedelta(days=180)): ts][\"unit_sales\"]\n",
        "        fig, ax = plt.subplots(figsize=(9, 4))\n",
        "        ax.plot(hist.index, hist.values, label=\"Actual (history)\")\n",
        "        ax.plot(fcst.index, fcst[\"prediction\"].values, marker=\"o\", label=\"Forecast\")\n",
        "        ax.axvline(ts, ls=\"--\", alpha=0.5)  # vertical line at cut-off date\n",
        "        ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Unit sales\"); ax.legend()\n",
        "        st.pyplot(fig, clear_figure=True)\n",
        "\n",
        "        # 6) Show forecast table and provide a CSV download\n",
        "        st.dataframe(fcst.rename_axis(\"date\"))\n",
        "        st.download_button(\n",
        "            \"Download forecast CSV\",\n",
        "            fcst.rename_axis(\"date\").to_csv(),\n",
        "            file_name=f\"forecast_{horizon}d_from_{ts.date()}.csv\",\n",
        "            mime=\"text/csv\",\n",
        "        )\n",
        "\n",
        "# Standard Python entry point so the script can be run directly\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I2wCCm7shLpw",
        "outputId": "96822aaf-c465-4db9-b31e-160a416444b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'311A4L0VfkmH66wbUVL1mAO3Aq0_3qCA1W8LaUPFWL1jgxS9n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('NGROK_AUTH_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb387998",
        "outputId": "e6f6903f-2244-477e-978b-9f4e3be3ddaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App URL: https://49134c80e0d7.ngrok-free.app\n",
            "Pyngrok setup for tunneling is ready.\n",
            "To run the Streamlit app, execute the command:\n",
            "!streamlit run /content/drive/MyDrive/corporacion_favorita/app/app.py\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import time # Import time for retries\n",
        "\n",
        "# Terminate open tunnels if any\n",
        "ngrok.kill()\n",
        "\n",
        "# Get the authtoken from ngrok dashboard\n",
        "# For Colab, it's recommended to add your authtoken to the Colab secrets manager\n",
        "# under the key 'NGROK_AUTH_TOKEN' and use userdata.get('NGROK_AUTH_TOKEN')\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open a HTTP tunnel on port 8501 (the default Streamlit port)\n",
        "# Make sure you have replaced 'YOUR_AUTHTOKEN' or set NGROK_AUTH_TOKEN in secrets\n",
        "# If you did not set NGROK_AUTH_TOKEN in secrets, uncomment the line above to set it directly\n",
        "# Replace with your actual authtoken or set it in Colab secrets\n",
        "try:\n",
        "    ngrok_tunnel = ngrok.connect(8501)\n",
        "    print(f\"Streamlit App URL: {ngrok_tunnel.public_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error opening ngrok tunnel: {e}\")\n",
        "\n",
        "# Note: The above ngrok setup requires an authtoken. If you prefer not to use one\n",
        "# you might be able to use an older version of ngrok or other tunneling methods.\n",
        "# However, using an authtoken is the recommended and more reliable approach.\n",
        "\n",
        "# To run the Streamlit app, you will use the !streamlit run command\n",
        "# For example: !streamlit run /content/drive/MyDrive/corporacion_favorita/app/main.py\n",
        "\n",
        "print(\"Pyngrok setup for tunneling is ready.\")\n",
        "print(\"To run the Streamlit app, execute the command:\")\n",
        "print(\"!streamlit run /content/drive/MyDrive/corporacion_favorita/app/app.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1Cj1JtPZGdT",
        "outputId": "cf2a3676-8bbd-491e-eb9c-783a21fe3607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.186.144.114:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-10-28 11:16:19.766573: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761650179.800956    1712 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761650179.811709    1712 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761650179.835921    1712 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761650179.835968    1712 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761650179.835976    1712 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761650179.835983    1712 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "/content/drive/MyDrive/corporacion_favorita/app/app.py:166: UserWarning: [11:16:42] WARNING: /workspace/src/collective/../data/../common/error_msg.h:83: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
            "configuration generated by an older version of XGBoost, please export the model by calling\n",
            "`Booster.save_model` from that version first, then load it back in current version. See:\n",
            "\n",
            "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
            "\n",
            "for more details about differences between saving model and serializing.\n",
            "\n",
            "  model = pickle.load(f)\n",
            "[Model loaded] Successfully loaded model from my_model.pkl\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!streamlit run /content/drive/MyDrive/corporacion_favorita/app/app.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IxdFADu8kpkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQA13PB2kqId"
      },
      "source": [
        "# %load /content/drive/MyDrive/corporacion_favorita/app/app.py\n",
        "# --- Make project packages importable (app, data, model) ---\n",
        "# Add the project root (…/corporacion_favorita) to sys.path so we can do:\n",
        "#   from app.config import ...\n",
        "#   from data.data_utils import ...\n",
        "#   from model.model_utils import ...\n",
        "import sys, os\n",
        "# Use the known project root path instead of __file__\n",
        "ROOT = '/content/drive/MyDrive/corporacion_favorita'\n",
        "if ROOT not in sys.path:\n",
        "    sys.path.insert(0, ROOT)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Core libs and UI\n",
        "import streamlit as st\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests, io # Added imports for data loading\n",
        "\n",
        "# Project config and modules\n",
        "# Importing from modules within the project structure\n",
        "from app.config import FILE_IDS, SEQ_LEN          # FILE_IDS has Drive file IDs; SEQ_LEN = LSTM window\n",
        "# Corrected import for data utility functions (removed import, functions called directly)\n",
        "# from data.data_utils import load_data, preprocess_input_data\n",
        "# Corrected import for model utility functions (removed import, functions called directly)\n",
        "# from model.model_utils import load_lstm_model, load_scaler_and_features, predict_scaled\n",
        "\n",
        "# Include data loading and preprocessing functions directly\n",
        "# ---------- Drive helpers ----------\n",
        "\n",
        "def make_drive_url(file_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Build a direct-download URL for a Google Drive file ID.\n",
        "    NOTE: The Drive file must be shared as “Anyone with the link can view”.\n",
        "    \"\"\"\n",
        "    return f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "def load_csv_from_url(url: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Download a CSV from a direct URL and read it into a DataFrame.\n",
        "    Uses requests for HTTP and StringIO to feed text to pandas.\n",
        "    \"\"\"\n",
        "    r = requests.get(url)\n",
        "    r.raise_for_status()                 # raise if HTTP request failed (e.g., 403/404)\n",
        "    return pd.read_csv(io.StringIO(r.text))\n",
        "\n",
        "# ---------- Data loading ----------\n",
        "\n",
        "def read_metadata_files(file_ids=FILE_IDS):\n",
        "    \"\"\"\n",
        "    Load the 5 metadata tables used for context/joins:\n",
        "      - holiday_events, items, oil, stores, transactions\n",
        "    Returns 5 DataFrames in that order.\n",
        "    \"\"\"\n",
        "    df_holiday_events = load_csv_from_url(make_drive_url(file_ids[\"holiday_events\"]))\n",
        "    df_items          = load_csv_from_url(make_drive_url(file_ids[\"items\"]))\n",
        "    df_oil            = load_csv_from_url(make_drive_url(file_ids[\"oil\"]))\n",
        "    df_stores         = load_csv_from_url(make_drive_url(file_ids[\"stores\"]))\n",
        "    df_transactions   = load_csv_from_url(make_drive_url(file_ids[\"transactions\"]))\n",
        "    return df_holiday_events, df_items, df_oil, df_stores, df_transactions\n",
        "\n",
        "def load_data(file_ids=FILE_IDS):\n",
        "    \"\"\"\n",
        "    Download metadata CSVs and the filtered training series (pickled DataFrame).\n",
        "\n",
        "    Returns (in order):\n",
        "        df_stores, df_items, df_transactions, df_oil, df_holiday_events, df_filtered\n",
        "    \"\"\"\n",
        "    # Load the five metadata tables\n",
        "    df_holiday_events, df_items, df_oil, df_stores, df_transactions = read_metadata_files(file_ids)\n",
        "\n",
        "    # Load the filtered time series (single store/item) from a pickled DataFrame on Drive.\n",
        "    # IMPORTANT: The Drive file must be public to read via direct link.\n",
        "    # Using a safer method to handle potential HTML responses from Drive links\n",
        "    url = make_drive_url(file_ids[\"train\"])\n",
        "    r = requests.get(url, allow_redirects=True); r.raise_for_status()\n",
        "    if r.content[:1] == b\"<\":\n",
        "        raise RuntimeError(\"Drive returned HTML. Check sharing or file ID.\")\n",
        "    df_filtered = pd.read_pickle(io.BytesIO(r.content))\n",
        "\n",
        "\n",
        "    return df_stores, df_items, df_transactions, df_oil, df_holiday_events, df_filtered\n",
        "\n",
        "# ---------- Feature engineering ----------\n",
        "\n",
        "def creating_features(df_filtered: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add features the LSTM expects.\n",
        "    Assumes:\n",
        "      - df_filtered has a daily DatetimeIndex\n",
        "      - 'unit_sales' column exists\n",
        "    \"\"\"\n",
        "    df = df_filtered.copy()\n",
        "\n",
        "    # Target lags: yesterday, last week, last month (by day count)\n",
        "    df[\"lag_1\"]  = df[\"unit_sales\"].shift(1)\n",
        "    df[\"lag_7\"]  = df[\"unit_sales\"].shift(7)\n",
        "    df[\"lag_30\"] = df[\"unit_sales\"].shift(30)\n",
        "    df.dropna(inplace=True)  # drop rows made NaN by the shifts above\n",
        "\n",
        "    # Calendar features derived from the index\n",
        "    df[\"day_of_week\"] = df.index.dayofweek     # 0=Mon ... 6=Sun\n",
        "    df[\"month\"]       = df.index.month\n",
        "    df[\"is_weekend\"]  = (df[\"day_of_week\"] >= 5).astype(int)\n",
        "\n",
        "    # Rolling stats on the target (shift by 1 to avoid peeking into the current day)\n",
        "    df[\"rolling_mean_7\"] = df[\"unit_sales\"].shift(1).rolling(window=7).mean()\n",
        "    df[\"rolling_std_7\"]  = df[\"unit_sales\"].shift(1).rolling(window=7).std()\n",
        "\n",
        "    # Drop any rows still NaN due to rolling window warm-up\n",
        "    df.dropna(inplace=True)\n",
        "    return df\n",
        "\n",
        "def preprocess_input_data(df_filtered: pd.DataFrame, pickled: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepare a DataFrame for inference.\n",
        "\n",
        "    If 'pickled' is True:\n",
        "      - assume df_filtered is already daily, indexed by date, and aggregated.\n",
        "\n",
        "    If 'pickled' is False:\n",
        "      - parse 'date' column to datetime\n",
        "      - aggregate to daily totals (numeric_only guards against non-numeric columns)\n",
        "      - set daily frequency and fill missing days with zeros\n",
        "      - then build features\n",
        "    \"\"\"\n",
        "    if not pickled:\n",
        "        df = df_filtered.copy()\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "        # Aggregate to daily totals (numeric_only guards against non-numeric columns)\n",
        "        df = df.groupby(\"date\").sum(numeric_only=True)[\"unit_sales\"].reset_index()\n",
        "        df.set_index(\"date\", inplace=True)\n",
        "        df = df.asfreq(\"D\").fillna(0)  # fill gaps with 0 sales\n",
        "    else:\n",
        "        df = df_filtered\n",
        "\n",
        "    return creating_features(df)\n",
        "\n",
        "# Including model loading and prediction functions directly\n",
        "import json\n",
        "import pickle\n",
        "import mlflow\n",
        "import mlflow.keras\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming these config variables are available in the environment or imported elsewhere\n",
        "# from app.config import MLFLOW_TRACKING_URI, MODEL_URI, MODEL_PATH, SCALER_PATH, FEATURES_JSON\n",
        "\n",
        "# Configuration variables (copied from cell Bjol6qVSXlpD)\n",
        "BASE_DIR = '/content/drive/MyDrive/corporacion_favorita' # Assuming this is your project root\n",
        "MLFLOW_TRACKING_URI = \"file:///content/drive/MyDrive/corporacion_favorita/mlflow_project\" # TODO: Update this path to the absolute path of your local mlflow_results folder.\n",
        "MODEL_URI = \"runs:/26a599ff4fe14c7fb552e9ed53534ecb/model\" # TODO: Replace <your_run_id> with the actual run ID from your MLflow experiment.\n",
        "MODEL_PATH    = os.path.join(BASE_DIR, \"models\", \"lstm_model.keras\")\n",
        "SCALER_PATH   = os.path.join(BASE_DIR, \"models\", \"scaler.pkl\")\n",
        "FEATURES_JSON = os.path.join(BASE_DIR, \"models\", \"feature_cols.json\")\n",
        "\n",
        "# Modified load_lstm_model to load from the pickled file\n",
        "def load_model():\n",
        "    \"\"\"\n",
        "    Load the model from the pickled file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use the variable defined here in app/main.py\n",
        "        with open('/content/drive/MyDrive/corporacion_favorita/my_model.pkl', 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        print(\"[Model loaded] Successfully loaded model from my_model.pkl\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"[Model load failed] Could not load model from my_model.pkl. Details: {e}\")\n",
        "        raise # Re-raise the exception to stop the app\n",
        "\n",
        "def load_scaler_and_features():\n",
        "    \"\"\"\n",
        "    Load the preprocessing artifacts required for inference:\n",
        "    - scaler.pkl : the fitted scaler used during training\n",
        "    - feature_cols.json : the exact feature order expected by the model\n",
        "    \"\"\"\n",
        "    # Use the variables defined here in app/main.py\n",
        "    with open(SCALER_PATH, \"rb\") as f:\n",
        "        scaler = pickle.load(f)\n",
        "    with open(FEATURES_JSON) as f:\n",
        "        feature_cols = json.load(f)\n",
        "    return scaler, feature_cols\n",
        "\n",
        "# Keep predict_scaled if the pickled model has a similar prediction interface\n",
        "# If the pickled model's prediction method is different, this function might need adjustment\n",
        "def predict_scaled(model, X_3d):\n",
        "    \"\"\"\n",
        "    Run the model on a 3D tensor shaped (batch, seq_len, n_features).\n",
        "    Returns a scalar (float) prediction in the **scaled space**.\n",
        "    \"\"\"\n",
        "    # Assuming the pickled model has a .predict method that takes a numpy array\n",
        "    # and returns a prediction that needs inverse scaling.\n",
        "    # This might need adjustment based on the actual pickled model object.\n",
        "    return model.predict(X_3d).ravel()[0]\n",
        "\n",
        "\n",
        "# Fixed series selection (the pickled df is already filtered to this pair)\n",
        "# These are just displayed in the UI; the actual df is already filtered.\n",
        "FIXED_STORE_ID = 44\n",
        "FIXED_ITEM_ID  = 1047679\n",
        "\n",
        "# Streamlit page setup\n",
        "st.set_page_config(page_title=\"Corporación Favorita — Sales Forecasting\", layout=\"wide\")\n",
        "\n",
        "# ---------- Helpers for multi-day forecasting ----------\n",
        "\n",
        "def _predict_next(model, scaler, feature_cols, history, seq_len):\n",
        "    \"\"\"\n",
        "    Predict next day's unit_sales using the last `seq_len` rows from `history`.\n",
        "\n",
        "    Steps:\n",
        "      1) take last seq_len rows and select the trained feature columns\n",
        "      2) scale with the saved scaler\n",
        "      3) reshape to 3D (batch, seq_len, n_features) and run the model\n",
        "      4) inverse-scale ONLY the target (assumed to be the FIRST column)\n",
        "    \"\"\"\n",
        "    window = history.tail(seq_len)\n",
        "    X_win = window[feature_cols].to_numpy()\n",
        "    X_scaled = scaler.transform(X_win)\n",
        "    # The pickled model might expect a different input shape than the LSTM (e.g., 2D instead of 3D)\n",
        "    # This part might need significant adjustment based on the pickled model type.\n",
        "    # For now, we assume it takes a 3D input like the LSTM.\n",
        "    X_3d = X_scaled.reshape(1, SEQ_LEN, len(feature_cols))\n",
        "    y_scaled = predict_scaled(model, X_3d)\n",
        "\n",
        "    # Inverse-scale just the target: place the scaled y in column 0, zeros elsewhere.\n",
        "    pad = np.zeros((1, len(feature_cols)))\n",
        "    pad[:, 0] = y_scaled\n",
        "    y = float(scaler.inverse_transform(pad)[0, 0])\n",
        "    return y\n",
        "\n",
        "def forecast_horizon(model, scaler, feature_cols, feats, start_date, seq_len, horizon=7):\n",
        "    \"\"\"\n",
        "    Autoregressive forecast for `horizon` days, starting the day *after* `start_date`.\n",
        "    Feed each prediction back into the working series to produce the next step.\n",
        "\n",
        "    `feats` is the engineered feature DataFrame with a DatetimeIndex and a 'unit_sales' column.\n",
        "    \"\"\"\n",
        "    # Work on a copy up to the chosen cut-off date\n",
        "    work = feats.loc[:start_date].copy()\n",
        "    preds = []\n",
        "    current = pd.to_datetime(start_date)\n",
        "\n",
        "    for _ in range(horizon):\n",
        "        # 1) Predict the next day using the current history window\n",
        "        y = _predict_next(model, scaler, feature_cols, work, seq_len)\n",
        "        next_date = current + pd.Timedelta(days=1)\n",
        "        preds.append((next_date, y))\n",
        "\n",
        "        # 2) Create the new row of engineered features for `next_date`\n",
        "        #    Note: rolling features are computed from past ACTUALS ONLY (no leakage).\n",
        "        last = work[\"unit_sales\"].iloc[-1]\n",
        "        last7  = work[\"unit_sales\"].tail(7)\n",
        "        last30 = work[\"unit_sales\"].tail(30)\n",
        "        new = {\n",
        "            \"unit_sales\":      y,                                # we append the prediction as if it were the next actual\n",
        "            \"lag_1\":           last,                             # yesterday's actual\n",
        "            \"lag_7\":           (work[\"unit_sales\"].iloc[-7]  if len(work) >= 7  else last),\n",
        "            \"lag_30\":          (work[\"unit_sales\"].iloc[-30] if len(work) >= 30 else last),\n",
        "            \"day_of_week\":     next_date.dayofweek,\n",
        "            \"month\":           next_date.month,\n",
        "            \"is_weekend\":      1 if next_date.dayofweek >= 5 else 0,\n",
        "            # Training used shift(1).rolling(7) → compute from past actuals only (no inclusion of y)\n",
        "            \"rolling_mean_7\":  last7.mean() if len(last7) >= 1 else np.nan,\n",
        "            \"rolling_std_7\":   last7.std(ddof=1) if len(last7) >= 2 else 0.0,\n",
        "        }\n",
        "        # Append the synthetic row to extend the working history and move forward one day\n",
        "        work = pd.concat([work, pd.DataFrame([new], index=[next_date])])\n",
        "        current = next_date\n",
        "\n",
        "    # Collect predictions into a DataFrame indexed by date\n",
        "    fcst = pd.DataFrame(preds, columns=[\"date\", \"prediction\"]).set_index(\"date\")\n",
        "    return fcst, work\n",
        "\n",
        "# ---------- App ----------\n",
        "\n",
        "def main():\n",
        "    st.title(\"Corporación Favorita — Sales Forecasting\")\n",
        "    st.caption(f\"Series: Store {FIXED_STORE_ID} · Item {FIXED_ITEM_ID} (fixed)\")\n",
        "\n",
        "    # 1) Data → engineered features\n",
        "    # Load metadata + filtered series from Drive, then build the LSTM features.\n",
        "    try:\n",
        "        _stores, _items, _tx, _oil, _hol, df_train = load_data(FILE_IDS)\n",
        "        feats = preprocess_input_data(df_train)  # adds lags/rolling/calendar features\n",
        "    except Exception as e:\n",
        "        # If you see this: check Drive sharing/IDs; the 'train' file must allow \"Anyone with the link can view\".\n",
        "        st.error(f\"Failed to load data. Check Drive sharing and FILE_IDS. Details:\\n{e}\")\n",
        "        st.stop() # Stop execution if data loading fails\n",
        "\n",
        "    # 2) Model + scaler + feature order\n",
        "    # Try to load from the pickled model file.\n",
        "    try:\n",
        "        model = load_model() # Changed from load_lstm_model\n",
        "        scaler, feature_cols = load_scaler_and_features()\n",
        "    except Exception as e:\n",
        "        st.error(f\"Failed to load model or artifacts. Details:\\n{e}\")\n",
        "        st.stop() # Stop execution if model/artifact loading fails\n",
        "\n",
        "    # 3) Date & horizon selection\n",
        "    # Choose a cut-off date (use history up to this date). Forecast starts the next day.\n",
        "    default_date = datetime.date(2014, 3, 1)\n",
        "    date = st.date_input(\n",
        "        \"Forecast cut‑off (use history up to this date)\",\n",
        "        value=default_date,\n",
        "        min_value=feats.index.min().date(),\n",
        "        max_value=feats.index.max().date(),\n",
        "    )\n",
        "    ts = pd.to_datetime(date)\n",
        "\n",
        "    # Single day vs multi-day (N) selection\n",
        "    mode = st.radio(\"Forecast mode\", [\"Single day\", \"Next N days\"], horizontal=True)\n",
        "    horizon = st.slider(\"N days\", 1, 30, 7) if mode == \"Next N days\" else 1\n",
        "\n",
        "    # 4) Predict\n",
        "    if st.button(\"Get Forecast\"):\n",
        "        # Validate the chosen date and ensure we have at least SEQ_LEN days of history\n",
        "        if ts not in feats.index:\n",
        "            st.error(\"Date out of range.\"); st.stop()\n",
        "        window = feats.loc[:ts].tail(SEQ_LEN)\n",
        "        if len(window) < SEQ_LEN:\n",
        "            st.error(f\"Need {SEQ_LEN} days of history before {date}.\"); st.stop()\n",
        "\n",
        "        if horizon == 1:\n",
        "            # Single-day forecast (original behavior)\n",
        "            X_win = window[feature_cols].to_numpy()\n",
        "            X_scaled = scaler.transform(X_win)\n",
        "            # The pickled model might expect a different input shape (e.g., 2D instead of 3D)\n",
        "            # This part might need significant adjustment based on the pickled model type.\n",
        "            X_input = X_scaled.reshape(1, SEQ_LEN, len(feature_cols)) # Assuming 3D input like LSTM\n",
        "            y_pred_scaled = predict_scaled(model, X_input)\n",
        "\n",
        "            # Inverse-scale only the target dimension\n",
        "            pad = np.zeros((1, len(feature_cols))); pad[:, 0] = y_pred_scaled\n",
        "            y_pred = float(scaler.inverse_transform(pad)[0, 0])\n",
        "\n",
        "            st.success(f\"Predicted sales for {date}: {y_pred:.2f}\")\n",
        "            # For consistency with plotting/table, put the result on the NEXT day\n",
        "            fcst = pd.DataFrame({\"prediction\": [y_pred]},\n",
        "                                index=[ts + pd.Timedelta(days=1)])\n",
        "            work = feats\n",
        "        else:\n",
        "            # Autoregressive N-day forecast\n",
        "            fcst, work = forecast_horizon(model, scaler, feature_cols, feats, ts, SEQ_LEN, horizon)\n",
        "            st.success(f\"Predicted {horizon} days: {fcst.index[0].date()} → {fcst.index[-1].date()}.\")\n",
        "\n",
        "        # 5) Plot: last ~6 months of history + forecast overlay\n",
        "        hist = feats.loc[max(feats.index.min(), ts - pd.Timedelta(days=180)): ts][\"unit_sales\"]\n",
        "        fig, ax = plt.subplots(figsize=(9, 4))\n",
        "        ax.plot(hist.index, hist.values, label=\"Actual (history)\")\n",
        "        ax.plot(fcst.index, fcst[\"prediction\"].values, marker=\"o\", label=\"Forecast\")\n",
        "        ax.axvline(ts, ls=\"--\", alpha=0.5)  # vertical line at cut-off date\n",
        "        ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Unit sales\"); ax.legend()\n",
        "        st.pyplot(fig, clear_figure=True)\n",
        "\n",
        "        # 6) Show forecast table and provide a CSV download\n",
        "        st.dataframe(fcst.rename_axis(\"date\"))\n",
        "        st.download_button(\n",
        "            \"Download forecast CSV\",\n",
        "            fcst.rename_axis(\"date\").to_csv(),\n",
        "            file_name=f\"forecast_{horizon}d_from_{ts.date()}.csv\",\n",
        "            mime=\"text/csv\",\n",
        "        )\n",
        "\n",
        "# Standard Python entry point so the script can be run directly\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f67b257a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45766369-013f-46fe-8cb5-4e51e573c5c9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.186.144.114:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!streamlit run /content/drive/MyDrive/corporacion_favorita/app/app.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3gfcExD6j11m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOx4thGKwOQguUXzxlGTDRd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}